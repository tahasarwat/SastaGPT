{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuQWFLwJ_01Y"
      },
      "source": [
        "# PA4 Part 1. Embeddings and Sentence Classification [30 Marks]\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/embeddings.jpeg\">\n",
        "</center>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Embeddings are a way to represent words (or more generally, *tokens*) as vectors. These vectors are useful for many tasks in NLP, including but not limited to: Text Generation, Machine Translation, and Sentence Classification. In this notebook, we will be exploring the concept of Embeddings, and using them for Sentence Classification.\n",
        "\n",
        "After this notebook, you should be able to:\n",
        "\n",
        "- Understand the concepts of Embeddings and Vector Similarity.\n",
        "\n",
        "- Use pre-trained Embeddings for Sentence Classification.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- Follow along with the notebook, filling out the necessary code where instructed.\n",
        "\n",
        "- <span style=\"color: red;\">Read the Submission Instructions and Plagiarism Policy in the attached PDF.</span>\n",
        "\n",
        "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
        "\n",
        "- <span style=\"color: red;\">Do not remove any pre-written code.</span> We will be using the `print` statements to grade you.\n",
        "\n",
        "- <span style=\"color: red;\">You must attempt all parts.</span> Do not assume that because something is for 0 marks, you can leave it - it will definitely be used in later parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dm4w_UhUhg_",
        "outputId": "2d257cf4-4a1b-4a27-eb99-0d6d4f557538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2K0KYyu_01l",
        "outputId": "3c9c294a-0000-4f69-fa1f-f16fff991f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.0.2-py3-none-manylinux1_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2023.11.17)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "!pip install gpt4all\n",
        "from gpt4all import Embed4All\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBfVHyvR_01v"
      },
      "source": [
        "## Exploring Embeddings [10 Marks]\n",
        "\n",
        "Put simply, Embeddings are fixed-size **dense** vector representations of tokens in natural language. This means you can represent words as vectors, sentences as vectors, even other entities like entire graphs as vectors.\n",
        "\n",
        "So what really makes them different from something like One-Hot vectors?\n",
        "\n",
        "What's special is that they have semantic meaning baked into them. This means you can model relationships between entities in text, which itself leads to a lot of fun applications. All modern architectures make use of Embeddings in some way.\n",
        "\n",
        "You can read more about them [here](https://aman.ai/primers/ai/word-vectors/).\n",
        "\n",
        "We will be using *pretrained* Embeddings: this means that we will be using Embeddings that have already been trained on a large corpus of text. This is because training Embeddings from scratch is a very computationally expensive task, and we don't have the resources to do so. Fortunately, there were some good samaritans who have already done this for us, and we can use their publicly available Embeddings for our own tasks.\n",
        "\n",
        "\n",
        "This part will allow you to explore what Embeddings are. We will load in pretrained Embeddings here and examine some of their properties. If you're interested, feel free look up the [Word2Vec model](https://arxiv.org/abs/1301.3781): this is the model that was trained to give us the embeddings you will see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKLk6HEb_01y",
        "outputId": "935a9455-0409-47f7-d64a-a58feb3da115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
            "Done loading word2vec model!\n"
          ]
        }
      ],
      "source": [
        "# Download the pretrained word2vec model (this may take a few minutes)\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "corpus = api.load('text8')\n",
        "w2vmodel = Word2Vec(corpus)\n",
        "\n",
        "print(\"Done loading word2vec model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgMzDtYa_010"
      },
      "source": [
        "Now that we've loaded in the Embeddings, we can create an Embedding **layer** in PyTorch, `nn.Embedding`, that will perform the processing step for us.\n",
        "\n",
        "Note in the following cell how there is a given **vocab size** and **embedding dimension** for the model: this is important to note because some sets of Embeddings may be defined for a large set of words (a large vocab), whereas older ones perhaps have a smaller set (a small vocab); the Embedding dimension essentially tells us how many *features* have been learned for a given word, that will allow us to perform further processing on top of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3fTbhn0_012",
        "outputId": "76a695be-297a-4f49-867b-944b8374a0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 71290\n",
            "Some of the words in the vocabulary:\n",
            "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two']\n",
            "Embedding dimension: 100\n"
          ]
        }
      ],
      "source": [
        "# Define embedding layer using gensim\n",
        "embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(w2vmodel.wv.vectors))\n",
        "\n",
        "# Get some information from the w2vmodel\n",
        "print(f\"Vocab size: {len(w2vmodel.wv.key_to_index)}\")\n",
        "\n",
        "print(f\"Some of the words in the vocabulary:\\n{list(w2vmodel.wv.key_to_index.keys())[:10]}\")\n",
        "\n",
        "print(f\"Embedding dimension: {w2vmodel.wv.vectors.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE4ubusa_014"
      },
      "source": [
        "Now, for a demonstration, we instantiate two words, turn them into numbers (encoding them via their index in the vocab), and pass them through the Embedding layer.\n",
        "\n",
        "Note how the resultant Embeddings both have the same shape: 1 word, and 100 elements in the vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoKlpUNF_016",
        "outputId": "682357d8-df5c-4fa2-8b42-4a8d6eb57385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Shape for 'king': torch.Size([1, 100])\n",
            "Embedding Shape for 'queen': torch.Size([1, 100])\n"
          ]
        }
      ],
      "source": [
        "# Take two words and get their embeddings\n",
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "\n",
        "def word2vec(word):\n",
        "    return embedding_layer(torch.LongTensor([w2vmodel.wv.key_to_index[word]]))\n",
        "\n",
        "king_embedding = word2vec(word1)\n",
        "queen_embedding = word2vec(word2)\n",
        "\n",
        "print(f\"Embedding Shape for '{word1}': {king_embedding.shape}\")\n",
        "print(f\"Embedding Shape for '{word2}': {queen_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDcf_HQv_017"
      },
      "source": [
        "When we have vectors whose scale is arbitrary, one nice way to measure how *similar* they are is with the Cosine Similarity measure.\n",
        "\n",
        "\n",
        "$$ \\text{Cosine Similarity}(\\mathbf{u},\\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} $$\n",
        "\n",
        "\n",
        "We can apply this idea to our Embeddings. To see how \"similar\" two words are to the model, we can generate their Embeddings and take the Cosine Similarity of them. This will be a number between -1 and 1 (just like the range of the cosine function). When the number is close to 0, the words are not similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlFue4zq_019",
        "outputId": "2510ddfb-aafd-487c-aae5-4d999a11aee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between 'good' and 'bad': 0.7351477742195129\n",
            "Similarity between 'good' and 'good': 1.0\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    '''\n",
        "    Computes the cosine similarity between two vectors\n",
        "    '''\n",
        "\n",
        "    # TODO: Compute the cosine similarity between the two vectors (using PyTorch)\n",
        "    dot_product = torch.sum(vec1 * vec2)\n",
        "    norm_vec1 = torch.norm(vec1)\n",
        "    norm_vec2 = torch.norm(vec2)\n",
        "\n",
        "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def compute_word_similarity(word1, word2):\n",
        "    '''\n",
        "    Takes in two words, computes their embeddings and returns the cosine similarity\n",
        "    '''\n",
        "    if word1 not in w2vmodel.wv or word2 not in w2vmodel.wv:\n",
        "        return \"One or both words not in vocabulary\"\n",
        "\n",
        "    embedding_word1 = embedding_layer(torch.LongTensor([w2vmodel.wv.key_to_index[word1]]))\n",
        "    embedding_word2 = embedding_layer(torch.LongTensor([w2vmodel.wv.key_to_index[word2]]))\n",
        "\n",
        "    similarity = cosine_similarity(embedding_word1, embedding_word2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# TODO: Define three words (one pair should be similar and one pair should be dissimilar) and compute their similarity\n",
        "word1 = 'good'\n",
        "word2 = 'bad'\n",
        "word3 = 'good'\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {compute_word_similarity(word1, word2)}\")\n",
        "print(f\"Similarity between '{word1}' and '{word3}': {compute_word_similarity(word1, word3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqzj_7aZ_01-"
      },
      "outputs": [],
      "source": [
        "# Run this cell if you're done with the above section\n",
        "del embedding_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhrHgOMZ_01_"
      },
      "source": [
        "## Sentence Classification Classification with Sentence Embeddings [20 Marks]\n",
        "\n",
        "Now let's move on to an actual application: classifying whether a tweet is about a real disaster or not. As you can imagine, this could be a valuable model when monitoring social media for disaster relief efforts.\n",
        "\n",
        "Since we are using Sentence Embeddings, we want something that will take in a sequence of words and throw out a single fixed-size vector. For this task, we will make use of an LLM via the `gpt4all` library.\n",
        "\n",
        "This library will allow us to generate pretrained embeddings for sentences, that we can use as **features** to feed to any classifier of our choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhRAdL_-_02A",
        "outputId": "c2049a53-85b9-4fdb-b455-e3873bcd2808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6090, 2) (1523, 2)\n"
          ]
        }
      ],
      "source": [
        "# Read in the data here\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML PA4/disaster_tweets.csv\")\n",
        "df = df[[\"text\", \"target\"]]\n",
        "\n",
        "# Split the data\n",
        "train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(train.shape, val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVeUWtCf_02B"
      },
      "source": [
        "Before jumping straight to Embeddings, since our data is sourced from the cesspool that is Twitter, we should probably do some cleaning. This can involve the removal of URLs, punctuation, numbers that don't provide any meaning, stopwords, and so on.'\n",
        "\n",
        "In the following cell, write functions to clean the sentences. You are allowed to add more functions if you wish, but the ones provided are the bare minimum.\n",
        "\n",
        "**Note:** After cleaning your sentences, it is possible that you may end up with empty sentences (or some that are so short they have lost all meaning). In this event, since we want to demonstrate setting up a Sentence Classification task, you should remove them from your dataset (data cleaning is not the center of this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhMJDBW1_02B",
        "outputId": "e4b1a2d3-b91f-405e-bcaf-02496a81e9db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6035, 3) (1509, 3)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Clean the sentences (5 marks)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# TODO: Fill out the following functions, adding more if desired\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def lowercase(txt):\n",
        "\n",
        "    return txt.lower()\n",
        "\n",
        "def remove_punctuation(txt):\n",
        "\n",
        "    return re.sub(r'[^\\w\\s]', '', txt)\n",
        "\n",
        "def remove_stopwords(txt):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(txt)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def remove_numbers(txt):\n",
        "\n",
        "    return re.sub(r'\\d+', '', txt)\n",
        "\n",
        "def remove_url(txt):\n",
        "\n",
        "    return re.sub(r'http\\S+', '', txt)\n",
        "\n",
        "def normalize_sentence(txt):\n",
        "    '''\n",
        "    Aggregates all the above functions to normalize/clean a sentence\n",
        "    '''\n",
        "    txt = lowercase(txt)\n",
        "    txt = remove_punctuation(txt)\n",
        "    txt = remove_stopwords(txt)\n",
        "    txt = remove_numbers(txt)\n",
        "    txt = remove_url(txt)\n",
        "    return txt\n",
        "\n",
        "def filter_short_sentences(df, min_length=20):\n",
        "    return df[df['text'].apply(lambda x: len(x) >= min_length)]\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML PA4/disaster_tweets.csv\")\n",
        "df = df[[\"text\", \"target\"]]\n",
        "\n",
        "\n",
        "# TODO: Clean the sentences\n",
        "df['clean_text'] = df['text'].apply(normalize_sentence)\n",
        "\n",
        "# TODO: Filter sentences that are too short (less than 20ish characters)\n",
        "df = filter_short_sentences(df)\n",
        "train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(train.shape, val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OUWEbyH_02C"
      },
      "source": [
        "Now for the fun part, creating our Embeddings!\n",
        "\n",
        "We will be using the `gpt4all.Embed4All` class for this purpose. You can look up the documentation [here](https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All.embed).\n",
        "\n",
        "This functionality makes use of a model called [Sentence-BERT](https://arxiv.org/abs/1908.10084). This is a Transformer-based model that has been trained on a large corpus of text, and is able to generate high-quality Sentence Embeddings for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbSW46nN_02C",
        "outputId": "4b33a5cf-07c3-4361-9b52-c210f309a497"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45.9M/45.9M [00:01<00:00, 32.8MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train embeddings shape: (6035, 384)\n",
            "Validation embeddings shape: (1509, 384)\n"
          ]
        }
      ],
      "source": [
        "from gpt4all import Embed4All\n",
        "\n",
        "# TODO: Generate embeddings for train and validation sentences (5 marks)\n",
        "feature_extractor = Embed4All()\n",
        "\n",
        "# TODO: Encode the train samples\n",
        "train_embeddings = [feature_extractor.embed(sentence) for sentence in train['clean_text'].tolist()]\n",
        "\n",
        "# TODO: Encode the train sentences\n",
        "train_embeddings = [feature_extractor.embed(sentence) for sentence in train['clean_text'].tolist()]\n",
        "\n",
        "# TODO: Encode the validation sentences\n",
        "val_embeddings = [feature_extractor.embed(sentence) for sentence in val['clean_text'].tolist()]\n",
        "\n",
        "train_embeddings = np.vstack(train_embeddings)\n",
        "val_embeddings = np.vstack(val_embeddings)\n",
        "\n",
        "# TODO: Ready the labels\n",
        "train_labels = train['target'].tolist()\n",
        "val_labels = val['target'].tolist()\n",
        "\n",
        "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
        "print(f\"Validation embeddings shape: {val_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worfsGc9_02D"
      },
      "source": [
        "Now with our Embeddings ready, we can move on to the actual classification task.\n",
        "\n",
        "You have the choice of using **any** classifier you wish. You can use a simple Logistic Regression model, get fancy with Support Vector Machines, or even use a Neural Network. The choice is yours.\n",
        "\n",
        "We will be looking for a model with a **Validation Accuracy** of around $0.8$. You must also use this model to make predictions on your own provided inputs, after completing the `predict` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tfNeTe1_02E",
        "outputId": "9e43772f-89f6-438c-bd42-73246e337aa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "18 fits failed out of a total of 36.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "18 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.79486285        nan 0.79386972        nan 0.78823578\n",
            "        nan 0.78608162        nan 0.78591603        nan 0.78541901]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7932\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       876\n",
            "           1       0.76      0.75      0.75       633\n",
            "\n",
            "    accuracy                           0.79      1509\n",
            "   macro avg       0.79      0.79      0.79      1509\n",
            "weighted avg       0.79      0.79      0.79      1509\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Get 0.8 Validation Acc with a Classifier (5 marks)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "val_embeddings_scaled = scaler.transform(val_embeddings)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=3)\n",
        "grid_search.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "best_classifier = grid_search.best_estimator_\n",
        "\n",
        "val_predictions = best_classifier.predict(val_embeddings_scaled)\n",
        "\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(val_labels, val_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxuaveYOZwSg",
        "outputId": "fe6fc51f-fc4e-4cae-d686-6ff265d3f9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: 'Life is like a race!'\n",
            "Prediction: 0, Probability: 0.3809\n",
            "\n",
            "Sentence: 'DO good have good.'\n",
            "Prediction: 0, Probability: 0.3730\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a function to predict on a sentence (5 marks)\n",
        "\n",
        "def predict(sentence, clf, feature_extractor):\n",
        "    '''\n",
        "    Takes in a sentence and returns the predicted class along with the probability\n",
        "    '''\n",
        "    # TODO: Clean and encode the sentence\n",
        "    cleaned_sentence = normalize_sentence(sentence)\n",
        "\n",
        "    embedding = feature_extractor.embed(cleaned_sentence)\n",
        "\n",
        "    embedding = np.array(embedding).reshape(1, -1)\n",
        "\n",
        "    # TODO: Predict the class and probability\n",
        "    prediction = clf.predict(embedding)\n",
        "    probability = clf.predict_proba(embedding)\n",
        "\n",
        "    return prediction, probability\n",
        "\n",
        "# TODO: Predict on a few of your own sentences\n",
        "sentence1 = \"Life is like a race!\"\n",
        "sentence2 = \"DO good have good.\"\n",
        "\n",
        "prediction1, probability1 = predict(sentence1, classifier, feature_extractor)\n",
        "prediction2, probability2 = predict(sentence2, classifier, feature_extractor)\n",
        "\n",
        "# Showing Result\n",
        "print(f\"Sentence: '{sentence1}'\\nPrediction: {prediction1[0]}, Probability: {probability1[0][1]:.4f}\")\n",
        "print()\n",
        "print(f\"Sentence: '{sentence2}'\\nPrediction: {prediction2[0]}, Probability: {probability2[0][1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHWicFsk_02H"
      },
      "source": [
        "Hopefully now you realize the power of Embeddings, and the usefulness of pretrained models.\n",
        "\n",
        "# Fin."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bbe381b710e5d3541ca1e32a0f143d44d9fc319722adcf51c48d4250c2e9fef8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
